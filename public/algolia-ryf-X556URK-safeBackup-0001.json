
[
  
  
  {
    "objectID": "1738368000",
    "permalink": "/post/%E7%AE%97%E6%B3%95/",
    "title": "算法",
    
    "content": " 参考链接 Hello 算法 (hello-algo.com)\n复杂度分析 迭代 迭代（iteration）是一种重复执行某个任务的控制结构。在迭代中，程序会在满足一定的条件下重复执行某段代码，直到这个条件不再满足。\nfor循环 for 循环是最常见的迭代形式之一，适合在预先知道迭代次数时使用。\n以下函数基于 for 循环实现了求和 1+2+⋯+𝑛 ，求和结果使用变量 res 记录。需要注意的是，Python 中 range(a, b) 对应的区间是“左闭右开”的，对应的遍历范围为 𝑎,𝑎+1,…,𝑏−1 ：\n/* for 循环 */ int forLoop(int n) { int res = 0; // 循环求和 1, 2, ..., n-1, n for (int i = 1; i \u0026lt;= n; i++) { res += i; } return res; } 此求和函数的操作数量与输入数据大小 𝑛 成正比，或者说成“线性关系”。实际上，时间复杂度描述的就是这个“线性关系”。\nwhile循环 与 for 循环类似，while 循环也是一种实现迭代的方法。在 while 循环中，程序每轮都会先检查条件，如果条件为真，则继续执行，否则就结束循环。\n下面我们用 while 循环来实现求和 1+2+⋯+𝑛 ：\n/* while 循环 */ int whileLoop(int n) { int res = 0; int i = 1; // 初始化条件变量 // 循环求和 1, 2, ..., n-1, n while (i \u0026lt;= n) { res += i; i++; // 更新条件变量 } return res; } while 循环比 for 循环的自由度更高。在 while 循环中，我们可以自由地设计条件变量的初始化和更新步骤。\n例如在以下代码中，条件变量 𝑖 每轮进行两次更新，这种情况就不太方便用 for 循环实现：\n/* while 循环（两次更新） */ int whileLoopII(int n) { int res = 0; int i = 1; // 初始化条件变量 // 循环求和 1, 4, 10, ... while (i \u0026lt;= n) { res += i; // 更新条件变量 i++; i *= 2; } return res; } 总的来说，for 循环的代码更加紧凑，while 循环更加灵活，两者都可以实现迭代结构。选择使用哪一个应该根据特定问题的需求来决定。\n嵌套循环 我们可以在一个循环结构内嵌套另一个循环结构，下面以 for 循环为例：\n/* 双层 for 循环 */ char *nestedForLoop(int n) { // n * n 为对应点数量，\u0026#34;(i, j), \u0026#34; 对应字符串长最大为 6+10*2，加上最后一个空字符 \\0 的额外空间 int size = n * n * 26 + 1; char *res = malloc(size * sizeof(char)); // 循环 i = 1, 2, ..., n-1, n for (int i = 1; i \u0026lt;= n; i++) { // 循环 j = 1, 2, ..., n-1, n for (int j = 1; j \u0026lt;= n; j++) { char tmp[26]; snprintf(tmp, sizeof(tmp), \u0026#34;(%d, %d), \u0026#34;, i, j); strncat(res, tmp, size - strlen(res) - 1); } } return res; } 在这种情况下，函数的操作数量与 $n^2$ 成正比，或者说算法运行时间和输入数据大小 𝑛 成“平方关系”。\n我们可以继续添加嵌套循环，每一次嵌套都是一次“升维”，将会使时间复杂度提高至“立方关系”“四次方关系”，以此类推。\n递归 递归（recursion）是一种算法策略，通过函数调用自身来解决问题。它主要包含两个阶段。\n递：程序不断深入地调用自身，通常传入更小或更简化的参数，直到达到“终止条件”。 归：触发“终止条件”后，程序从最深层的递归函数开始逐层返回，汇聚每一层的结果。 而从实现的角度看，递归代码主要包含三个要素。\n终止条件：用于决定什么时候由“递”转“归”。 递归调用：对应“递”，函数调用自身，通常输入更小或更简化的参数。 返回结果：对应“归”，将当前递归层级的结果返回至上一层。 观察以下代码，我们只需调用函数 recur(n) ，就可以完成 1+2+⋯+𝑛 的计算：\n/* 递归 */ int recur(int n) { // 终止条件 if (n == 1) return 1; // 递：递归调用 int res = recur(n - 1); // 归：返回结果 return n + res; } 虽然从计算角度看，迭代与递归可以得到相同的结果，但它们代表了两种完全不同的思考和解决问题的范式。\n迭代：“自下而上”地解决问题。从最基础的步骤开始，然后不断重复或累加这些步骤，直到任务完成。 递归：“自上而下”地解决问题。将原问题分解为更小的子问题，这些子问题和原问题具有相同的形式。接下来将子问题继续分解为更小的子问题，直到基本情况时停止（基本情况的解是已知的）。 以上述求和函数为例，设问题 𝑓(𝑛)=1+2+⋯+𝑛 。\n迭代：在循环中模拟求和过程，从 1 遍历到 𝑛 ，每轮执行求和操作，即可求得 𝑓(𝑛) 。 递归：将问题分解为子问题 𝑓(𝑛)=𝑛+𝑓(𝑛−1) ，不断（递归地）分解下去，直至基本情况 𝑓(1)=1 时终止。 调用栈 递归函数每次调用自身时，系统都会为新开启的函数分配内存，以存储局部变量、调用地址和其他信息等。这将导致两方面的结果。\n函数的上下文数据都存储在称为“栈帧空间”的内存区域中，直至函数返回后才会被释放。因此，递归通常比迭代更加耗费内存空间。 递归调用函数会产生额外的开销。因此递归通常比循环的时间效率更低。 在触发终止条件前，同时存在 𝑛 个未返回的递归函数，递归深度为 𝑛 。\n在实际中，编程语言允许的递归深度通常是有限的，过深的递归可能导致栈溢出错误。\n尾递归 有趣的是，如果函数在返回前的最后一步才进行递归调用，则该函数可以被编译器或解释器优化，使其在空间效率上与迭代相当。这种情况被称为尾递归（tail recursion）。\n普通递归：当函数返回到上一层级的函数后，需要继续执行代码，因此系统需要保存上一层调用的上下文。 尾递归：递归调用是函数返回前的最后一个操作，这意味着函数返回到上一层级后，无须继续执行其他操作，因此系统无须保存上一层函数的上下文。 以计算 1+2+⋯+𝑛 为例，我们可以将结果变量 res 设为函数参数，从而实现尾递归：\n/* 尾递归 */ int tailRecur(int n, int res) { // 终止条件 if (n == 0) return res; // 尾递归调用 return tailRecur(n - 1, res + n); } 对比普通递归和尾递归，两者的求和操作的执行点是不同的。\n普通递归：求和操作是在“归”的过程中执行的，每层返回后都要再执行一次求和操作。 尾递归：求和操作是在“递”的过程中执行的，“归”的过程只需层层返回。 [!tip]\n请注意，许多编译器或解释器并不支持尾递归优化。例如，Python 默认不支持尾递归优化，因此即使函数是尾递归形式，仍然可能会遇到栈溢出问题。\n递归树 当处理与“分治”相关的算法问题时，递归往往比迭代的思路更加直观、代码更加易读。以“斐波那契数列”为例。\n问题：\n给定一个斐波那契数列0,1,2,3,4\u0026hellip;\u0026hellip;,求该数列的第n个数字\n设斐波那契数列的第 𝑛 个数字为 𝑓(𝑛) ，易得两个结论。\n数列的前两个数字为 𝑓(1)=0 和 𝑓(2)=1 。 数列中的每个数字是前两个数字的和，即 𝑓(𝑛)=𝑓(𝑛−1)+𝑓(𝑛−2) 。 按照递推关系进行递归调用，将前两个数字作为终止条件，便可写出递归代码。调用 fib(n) 即可得到斐波那契数列的第 𝑛 个数字：\n/* 斐波那契数列：递归 */ int fib(int n) { // 终止条件 f(1) = 0, f(2) = 1 if (n == 1 || n == 2) return n - 1; // 递归调用 f(n) = f(n-1) + f(n-2) int res = fib(n - 1) + fib(n - 2); // 返回结果 f(n) return res; } 观察以上代码，我们在函数内递归调用了两个函数，这意味着从一个调用产生了两个调用分支。这样不断递归调用下去，最终将产生一棵层数为 𝑛 的递归树（recursion tree）。\n从本质上看，递归体现了“将问题分解为更小子问题”的思维范式，这种分治策略至关重要。\n从算法角度看，搜索、排序、回溯、分治、动态规划等许多重要算法策略直接或间接地应用了这种思维方式。 从数据结构角度看，递归天然适合处理链表、树和图的相关问题，因为它们非常适合用分治思想进行分析。 时间复杂度 运行时间可以直观且准确地反映算法的效率。如果我们想准确预估一段代码的运行时间，应该如何操作呢？\n确定运行平台，包括硬件配置、编程语言、系统环境等，这些因素都会影响代码的运行效率。 评估各种计算操作所需的运行时间，例如加法操作 + 需要 1 ns ，乘法操作 * 需要 10 ns ，打印操作 print() 需要 5 ns 等。 统计代码中所有的计算操作，并将所有操作的执行时间求和，从而得到运行时间。 例如在以下代码中，输入数据大小为 𝑛 ：\n// 在某运行平台下 void algorithm(int n) { int a = 2; // 1 ns a = a + 1; // 1 ns a = a * 2; // 10 ns // 循环 n 次 for (int i = 0; i \u0026lt; n; i++) { // 1 ns printf(\u0026#34;%d\u0026#34;, 0); // 5 ns } } $$ 1+1+10+(1+5)×𝑛=6𝑛+12 $$ 但实际上，统计算法的运行时间既不合理也不现实。首先，我们不希望将预估时间和运行平台绑定，因为算法需要在各种不同的平台上运行。其次，我们很难获知每种操作的运行时间，这给预估过程带来了极大的难度。\n统计时间增长趋势 时间复杂度分析统计的不是算法运行时间，==而是算法运行时间随着数据量变大时的增长趋势==。\n“时间增长趋势”这个概念比较抽象，我们通过一个例子来加以理解。假设输入数据大小为 𝑛 ，给定三个算法 A、B 和 C ：\n// 算法 A 的时间复杂度：常数阶 void algorithm_A(int n) { printf(\u0026#34;%d\u0026#34;, 0); } // 算法 B 的时间复杂度：线性阶 void algorithm_B(int n) { for (int i = 0; i \u0026lt; n; i++) { printf(\u0026#34;%d\u0026#34;, 0); } } // 算法 C 的时间复杂度：常数阶 void algorithm_C(int n) { for (int i = 0; i \u0026lt; 1000000; i++) { printf(\u0026#34;%d\u0026#34;, 0); } } 三个函数的时间复杂度为：\n算法 A 只有 1 个打印操作，算法运行时间不随着 𝑛 增大而增长。我们称此算法的时间复杂度为“常数阶”。 算法 B 中的打印操作需要循环 𝑛 次，算法运行时间随着 𝑛 增大呈线性增长。此算法的时间复杂度被称为“线性阶”。 算法 C 中的打印操作需要循环 1000000 次，虽然运行时间很长，但它与输入数据大小 𝑛 无关。因此 C 的时间复杂度和 A 相同，仍为“常数阶”。 相较于直接统计算法的运行时间，时间复杂度分析有哪些特点呢？\n时间复杂度能够有效评估算法效率。例如，算法 B 的运行时间呈线性增长，在 𝑛\u0026gt;1 时比算法 A 更慢，在 𝑛\u0026gt;1000000 时比算法 C 更慢。事实上，只要输入数据大小 𝑛 足够大，复杂度为“常数阶”的算法一定优于“线性阶”的算法，这正是时间增长趋势的含义。 时间复杂度的推算方法更简便。显然，运行平台和计算操作类型都与算法运行时间的增长趋势无关。因此在时间复杂度分析中，我们可以简单地将所有计算操作的执行时间视为相同的“单位时间”，从而将“计算操作运行时间统计”简化为“计算操作数量统计”，这样一来估算难度就大大降低了。 时间复杂度也存在一定的局限性。例如，尽管算法 A 和 C 的时间复杂度相同，但实际运行时间差别很大。同样，尽管算法 B 的时间复杂度比 C 高，但在输入数据大小 𝑛 较小时，算法 B 明显优于算法 C 。对于此类情况，我们时常难以仅凭时间复杂度判断算法效率的高低。当然，尽管存在上述问题，复杂度分析仍然是评判算法效率最有效且常用的方法。 函数渐进上界 给定一个输入大小为n的函数：\nvoid algorithm(int n) { int a = 1; // +1 a = a + 1; // +1 a = a * 2; // +1 // 循环 n 次 for (int i = 0; i \u0026lt; n; i++) { // +1（每轮都执行 i ++） printf(\u0026#34;%d\u0026#34;, 0); // +1 } } $$ 𝑇(𝑛)=3+2𝑛 $$ 𝑇(𝑛) 是一次函数，说明其运行时间的增长趋势是线性的，因此它的时间复杂度是线性阶。\n我们将线性阶的时间复杂度记为 𝑂(𝑛) ，这个数学符号称为大 𝑂 记号（big-𝑂 notation），表示函数 𝑇(𝑛) 的渐近上界（asymptotic upper bound）。\n时间复杂度分析本质上是计算“操作数量 𝑇(𝑛)”的渐近上界，它具有明确的数学定义。\n[!TIP]\n若存在正实数 𝑐 和实数 𝑛0 ，使得对于所有的 𝑛\u0026gt;𝑛0 ，均有 𝑇(𝑛)≤𝑐⋅𝑓(𝑛) ，则可认为 𝑓(𝑛) 给出了 𝑇(𝑛) 的一个渐近上界，记为 𝑇(𝑛)=𝑂(𝑓(𝑛)) 。\n计算渐近上界就是寻找一个函数 𝑓(𝑛) ，使得当 𝑛 趋向于无穷大时，𝑇(𝑛) 和 𝑓(𝑛) 处于相同的增长级别，仅相差一个常数项 𝑐 的倍数。\n推算方法 渐近上界的数学味儿有点重，如果你感觉没有完全理解，也无须担心。我们可以先掌握推算方法，在不断的实践中，就可以逐渐领悟其数学意义。\n根据定义，确定 𝑓(𝑛) 之后，我们便可得到时间复杂度 𝑂(𝑓(𝑛)) 。那么如何确定渐近上界 𝑓(𝑛) 呢？总体分为两步：首先统计操作数量，然后判断渐近上界。\n统计操作数量 针对代码，逐行从上到下计算即可。然而，由于上述 𝑐⋅𝑓(𝑛) 中的常数项 𝑐 可以取任意大小，因此操作数量 𝑇(𝑛) 中的各种系数、常数项都可以忽略。根据此原则，可以总结出以下计数简化技巧。\n忽略 𝑇(𝑛) 中的常数项。因为它们都与 𝑛 无关，所以对时间复杂度不产生影响。 省略所有系数。例如，循环 2𝑛 次、5𝑛+1 次等，都可以简化记为 𝑛 次，因为 𝑛 前面的系数对时间复杂度没有影响。 循环嵌套时使用乘法。总操作数量等于外层循环和内层循环操作数量之积，每一层循环依然可以分别套用第 1. 点和第 2. 点的技巧。 给定一个函数，我们可以用上述技巧来统计操作数量：\nvoid algorithm(int n) { int a = 1; // +0（技巧 1） a = a + n; // +0（技巧 1） // +n（技巧 2） for (int i = 0; i \u0026lt; 5 * n + 1; i++) { printf(\u0026#34;%d\u0026#34;, 0); } // +n*n（技巧 3） for (int i = 0; i \u0026lt; 2 * n; i++) { for (int j = 0; j \u0026lt; n + 1; j++) { printf(\u0026#34;%d\u0026#34;, 0); } } } $$ T(n)=2n(n+1)+(5n+1)+2=2n^2+7n+3\\\\ T(n)=n^2+n $$ 判断渐近上界 时间复杂度由 𝑇(𝑛) 中最高阶的项来决定。这是因为在 𝑛 趋于无穷大时，最高阶的项将发挥主导作用，其他项的影响都可以忽略。\n下表展示了一些栗子，其中一些夸张的值是为了强调“系数无法撼动阶数”这一结论。当 𝑛 趋于无穷大时，这些常数变得无足轻重。\n操作数量T(n) 时间复杂度 O(f(n)) 100000 O(1) 3n+2 O(n) 2$n^2$+ 3n + 2 O($n^2$) $n^2$+10000$n^2$ O(n) $2^n+10000n^{10000}$ O($2^n$) 常见类型 $$ O(1)",
    
    "date": "2025-02-01 00:00:00",
    "updated": "2025-03-01 00:00:00"
  }
  
]